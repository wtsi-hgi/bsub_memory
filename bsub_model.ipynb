{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Path to input and output\n",
    "input_path = \"/Users/dn10/Downloads/Bsub_dataset/data.jsonl.gz\"\n",
    "output_path = \"/Users/dn10/Downloads/Bsub_dataset/filtered_under_5GB.jsonl\"\n",
    "\n",
    "# Target size limit (in bytes) - 5GB\n",
    "size_limit = 5 * 1024 * 1024 * 1024  # 5 GB\n",
    "\n",
    "# Open output file for writing\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    for chunk in pd.read_json(input_path, \n",
    "                              lines=True, \n",
    "                              compression='gzip', \n",
    "                              chunksize=100000):\n",
    "\n",
    "        # Filter rows that contain #BSUB in 'Command' column\n",
    "        filtered_chunk = chunk[chunk['Command'].str.contains('#BSUB', case=False, na=False)]\n",
    "\n",
    "        # Write to file in JSONL format\n",
    "        filtered_chunk.to_json(output_file, orient='records', lines=True)\n",
    "\n",
    "        # Check file size after writing\n",
    "        current_size = os.path.getsize(output_path)\n",
    "        print(f\"Written so far: {round(current_size / (1024 ** 2), 2)} MB\")\n",
    "\n",
    "        if current_size >= size_limit:\n",
    "            print(\"File size limit reached. Stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(output_path, lines=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a histogram\n",
    "plt.hist(df[\"MAX_MEM_USAGE_MB\"], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install seaborn\n",
    "sns.boxplot(x=df['MAX_MEM_USAGE_MB'])\n",
    "plt.xscale('log')  # Optional: helps if values vary a lot\n",
    "plt.title(\"Boxplot of Memory Usage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MAX_MEM_USAGE_MB'].quantile([0.25, 0.5, 0.75, 0.90, 0.95, 0.99, 0.999])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, column= \"MAX_MEM_USAGE_MB\", min_mem_mb = 1.0, quantile = 0.99, bins =100, samples_per_bin=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess the data by filtering and binning.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        column (str): Column name to filter on.\n",
    "        min_mem_mb (float): Minimum memory usage in MB.\n",
    "        quantile (float): Quantile to filter on.\n",
    "        bins (int): Number of bins for histogram.\n",
    "        samples_per_bin (int): Number of samples per bin.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    # Filter jobs with low memory\n",
    "    df = df[df[column] >= min_mem_mb].copy()\n",
    "    \n",
    "    # Calculate the upper bound using quantile\n",
    "    upper_bound = df[column].quantile(quantile)\n",
    "    \n",
    "    # Further filter rows based on the upper bound\n",
    "    df = df[df[column] <= upper_bound].reset_index(drop=True)\n",
    "    \n",
    "    # Bin the data\n",
    "    #df['bin'] = pd.qcut(df[column], q=bins, duplicates='drop')\n",
    "    df['bin'] = pd.cut(df[column], bins=bins, duplicates='drop')\n",
    "    \n",
    "    # Sample from each bin\n",
    "    sampled_df = df.groupby('bin').apply(lambda x: x.sample(min(len(x), samples_per_bin), random_state=random_state))\n",
    "    \n",
    "    # drop the bin column\n",
    "    sampled_df = sampled_df.reset_index(drop=True)\n",
    "    sampled_df = sampled_df.drop(columns=['bin'])\n",
    "    # Reset index\n",
    "    sampled_df = sampled_df.reset_index(drop=True)\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = preprocess_data(df, column=\"MAX_MEM_USAGE_MB\", min_mem_mb=1.0, quantile=0.99, bins=100, samples_per_bin=1000, random_state=42)\n",
    "print(f\"Number of rows after preprocessing: {len(df_balanced)}\")\n",
    "df_balanced.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced['MAX_MEM_USAGE_MB'].hist(bins=50, edgecolor='black')\n",
    "#groupby;sampling;reformatting.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-TinyBERT-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-TinyBERT-L6-v2')\n",
    "encoded_input = tokenizer(df['Command'][0], padding=True, truncation=True, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "print(encoded_input[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each command in the DataFrame\n",
    "def get_embedding(command):\n",
    "    encoded_input = tokenizer(command, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    # Perform mean pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    \n",
    "    # Return the sentence embedding (flattened to 1D tensor)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the embedding function to each row in the DataFrame\n",
    "df_balanced[\"Embeddings\"] = df_balanced[\"Command\"].apply(lambda x: get_embedding(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Storing df with embeddings in a pickle file\n",
    "with open('df_embeddings.pickle','wb') as file:\n",
    "    pickle.dump(df_balanced,file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('df_embeddings.pickle', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_balanced.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.MAX_MEM_USAGE_MB.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_balanced['MAX_MEM_USAGE_MB'] > 10000).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, test_size=0.2, random_state=42):\n",
    "    X = np.array([np.array([tensor.item() for tensor in emb[0]]) for emb in df[\"Embeddings\"]])\n",
    "    y = np.log1p(df[\"MAX_MEM_USAGE_MB\"].values)\n",
    "    \n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "X_train, X_test, y_train, y_test = prepare_data(df_balanced, test_size=0.2, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(X_train, y_train):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "model = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, X, y, dataset_label=\"\"):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = root_mean_squared_error(y, y_pred) \n",
    "\n",
    "    print(f\"{dataset_label} RÂ²: {r2:.4f}\")\n",
    "    print(f\"{dataset_label} RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return y_pred\n",
    "y_train_pred = evaluate_model(model, X_train, y_train, \"Train\")\n",
    "y_test_pred = evaluate_model(model, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(y_true, y_pred, label=\"\"):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.3, label=label)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--', label='Ideal')\n",
    "    plt.xlabel(\"Actual (log)\")\n",
    "    plt.ylabel(\"Predicted (log)\")\n",
    "    plt.title(f\"{label} - Predicted vs Actual\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_results(y_train, y_train_pred, \"Train\")\n",
    "plot_results(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample predictions (in MB):\")\n",
    "print(\"Predicted:\", np.round(np.expm1(y_test_pred[:5]), 2))\n",
    "print(\"Actual:   \", np.round(np.expm1(y_test[:5]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Linear Regression with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a Pytorch Dataset class for the embeddings\n",
    "\n",
    "class CommandDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            df(pd.DataFrame): DataFrame containing entire dataset with embeddings\n",
    "        \"\"\"\n",
    "        self.commands = df[\"Command\"]\n",
    "        '''\n",
    "\n",
    "        For efficiency, we convert the columns with numbers to tensors of float32 type.\n",
    "\n",
    "        '''\n",
    "        self.memory_requested = torch.tensor(df[\"MEM_REQUESTED_MB\"].values, dtype=torch.float32)\n",
    "        self.memory_used = torch.tensor(df[\"MAX_MEM_USAGE_MB\"].values, dtype=torch.float32)\n",
    "        self.num_of_processors = torch.tensor(df[\"NUM_EXEC_PROCS\"].values,dtype=torch.float32)\n",
    "        '''\n",
    "\n",
    "        Assuming we have tensor of embeddings\n",
    "\n",
    "        ''' \n",
    "        #print(df[\"Embeddings\"][0])\n",
    "        self.embeddings = df[\"Embeddings\"]\n",
    "        \n",
    "\n",
    "\n",
    "        print(f\"Initialized CommandDataset with {len(self)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "\n",
    "        Returns the length in the dataset/ number of commands\n",
    "\n",
    "        '''\n",
    "        return len(self.commands)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "\n",
    "        Returns the item at the given index\n",
    "\n",
    "        Args:\n",
    "            idx(int): index of the item to return\n",
    "        Returns:\n",
    "            dict: Dictionary containing the command, memory requested, memory used, number of processors, and embeddings\n",
    "\n",
    "        '''\n",
    "        commands = self.commands.iloc[idx]\n",
    "        memory_requested = self.memory_requested[idx]\n",
    "        memory_used = self.memory_used[idx]\n",
    "        num_of_processors = self.num_of_processors[idx]\n",
    "        embeddings = self.embeddings[idx]\n",
    "\n",
    "        data = {\n",
    "            \"command\": commands,\n",
    "            \"memory_requested\": memory_requested,\n",
    "            \"memory_used\": memory_used,\n",
    "            \"num_of_processors\": num_of_processors,\n",
    "            \"embeddings\": embeddings\n",
    "        }\n",
    "\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CommandDataset(df_balanced)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_with_labels(dataset, test_size=0.2):\n",
    "    # Calculate the number of samples for train and test\n",
    "    train_size = int(len(dataset) * (1 - test_size))\n",
    "    test_size = len(dataset) - train_size\n",
    "    \n",
    "    # Perform the random split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    print(f\"Dataset split into {len(train_dataset)} training samples and {len(test_dataset)} testing samples\")\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = split_dataset_with_labels(data, test_size=0.2)\n",
    "\n",
    "# Print the object types to verify\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the training set\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "# Create a DataLoader for the testing set\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = [batch[\"embeddings\"] for batch in train_loader]\n",
    "print(all_batches[0].shape)  # Access the shape of the first tensor directly\n",
    "\n",
    "batches = torch.cat(all_batches).squeeze(1)  # Squeeze the second dimension after concatenation\n",
    "print(batches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    embedding_size = data[i][\"embeddings\"].size(1)\n",
    "    print(f\"Embedding size for row {i}: {embedding_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Linear Regression Model\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pred_y = self.linear(x)\n",
    "        return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "input_dim = data[0][\"embeddings\"].size(1)\n",
    "output_dim = 1\n",
    "model = LinearRegression(input_dim,output_dim)\n",
    "\n",
    "#Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#Define the optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "model.train()\n",
    "losses = []\n",
    "for epoch in range(50):\n",
    "    for batch in train_loader:\n",
    "        #Forward pass\n",
    "        # Reshape embeddings to ensure they are 2D\n",
    "        pred_y = model(batch[\"embeddings\"])\n",
    "\n",
    "        #Loss calculation\n",
    "        loss = criterion(pred_y, batch[\"memory_used\"])\n",
    "        #append the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        #Backward pass\n",
    "        #Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update the weights\n",
    "        optimizer.step()\n",
    "        print (f\" Epoch { epoch} Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.linear.weight.grad)  # View gradients before zeroing\n",
    "optimizer.zero_grad()\n",
    "print(model.linear.weight.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the weights and bias\n",
    "weights = model.linear.weight.data\n",
    "bias = model.linear.bias.data\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias: {bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"After step - {name}: {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '/Users/dn10/Downloads/Bsub_dataset/model.pth') # pytorch function like pickle which saves an object to a file\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model.load_state_dict(torch.load('/Users/dn10/Downloads/Bsub_dataset/model.pth'))\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = nn.MSELoss()  \n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = 0\n",
    "total_predictions = numpy.array([])\n",
    "total_actual = numpy.array([])\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader:\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        pred_y = model(batch[\"embeddings\"])\n",
    "\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(pred_y, batch[\"memory_used\"])\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        test_loss += loss.item()\n",
    "        pred_y = pred_y.squeeze()\n",
    "        print(f\"Predicted: {pred_y}, Actual: {batch['memory_used']}\")\n",
    "        print(pred_y.shape)\n",
    "        print(batch['memory_used'].shape)\n",
    "        total_predictions = numpy.append(total_predictions, pred_y.numpy())\n",
    "        total_actual = numpy.append(total_actual,batch['memory_used'].numpy())\n",
    "# Average test loss\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(total_predictions)\n",
    "print(total_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot a scatter plot for total prediction and total actual\n",
    "plt.scatter(total_predictions, total_actual, alpha=0.5, s= 20)\n",
    "plt.xlabel('Predicted memory used')\n",
    "plt.ylabel('Actual memory used')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
