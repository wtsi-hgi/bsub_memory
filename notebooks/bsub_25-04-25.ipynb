{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import os\n",
    "\n",
    "# Path to input and output\n",
    "input_path = \"/Users/dn10/Downloads/Bsub_dataset/data.jsonl.gz\"\n",
    "output_path = \"/Users/dn10/Downloads/Bsub_dataset/filtered_under_5GB.jsonl\"\n",
    "\n",
    "# Target size limit (in bytes) - 5GB\n",
    "size_limit = 5 * 1024 * 1024 * 1024  # 5 GB\n",
    "\n",
    "# Open output file for writing\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    for chunk in pd.read_json(input_path, \n",
    "                              lines=True, \n",
    "                              compression='gzip', \n",
    "                              chunksize=100000):\n",
    "\n",
    "        # Filter rows that contain #BSUB in 'Command' column\n",
    "        filtered_chunk = chunk[chunk['Command'].str.contains('#BSUB', case=False, na=False)]\n",
    "\n",
    "        # Write to file in JSONL format\n",
    "        filtered_chunk.to_json(output_file, orient='records', lines=True)\n",
    "\n",
    "        # Check file size after writing\n",
    "        current_size = os.path.getsize(output_path)\n",
    "        print(f\"Written so far: {round(current_size / (1024 ** 2), 2)} MB\")\n",
    "\n",
    "        if current_size >= size_limit:\n",
    "            print(\"File size limit reached. Stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(output_path, lines=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(x=df['MAX_MEM_USAGE_MB'])\n",
    "plt.xscale('log')  # Optional: helps if values vary a lot\n",
    "plt.title(\"Boxplot of Memory Usage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MAX_MEM_USAGE_MB'].quantile([0.25, 0.5, 0.75, 0.90, 0.95, 0.99, 0.999])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, column= \"MAX_MEM_USAGE_MB\", min_mem_mb = 1.0, quantile = 0.99, bins =100, samples_per_bin=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess the data by filtering and binning.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        column (str): Column name to filter on.\n",
    "        min_mem_mb (float): Minimum memory usage in MB.\n",
    "        quantile (float): Quantile to filter on.\n",
    "        bins (int): Number of bins for histogram.\n",
    "        samples_per_bin (int): Number of samples per bin.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    # Filter jobs with low memory\n",
    "    df = df[df[column] >= min_mem_mb].copy()\n",
    "    \n",
    "    # Calculate the upper bound using quantile\n",
    "    upper_bound = df[column].quantile(quantile)\n",
    "    \n",
    "    # Further filter rows based on the upper bound\n",
    "    df = df[df[column] <= upper_bound].reset_index(drop=True)\n",
    "    \n",
    "    # Bin the data\n",
    "    df['bin'] = pd.cut(df[column], bins=bins, duplicates='drop')\n",
    "    \n",
    "    # Sample from each bin\n",
    "    sampled_df = df.groupby('bin').apply(lambda x: x.sample(min(len(x), samples_per_bin), random_state=random_state))\n",
    "    \n",
    "    # drop the bin column\n",
    "    sampled_df = sampled_df.reset_index(drop=True)\n",
    "    sampled_df = sampled_df.drop(columns=['bin'])\n",
    "    # Reset index\n",
    "    sampled_df = sampled_df.reset_index(drop=True)\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = preprocess_data(df, column=\"MAX_MEM_USAGE_MB\", min_mem_mb=1.0, quantile=0.99, bins=100, samples_per_bin=1000, random_state=42)\n",
    "print(f\"Number of rows after preprocessing: {len(df_balanced)}\")\n",
    "df_balanced.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced['MAX_MEM_USAGE_MB'].hist(bins=50, edgecolor='black')\n",
    "#groupby;sampling;reformatting.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-TinyBERT-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-TinyBERT-L6-v2')\n",
    "encoded_input = tokenizer(df['Command'][0], padding=True, truncation=True, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "print(encoded_input[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each command in the DataFrame\n",
    "def get_embedding(command):\n",
    "    encoded_input = tokenizer(command, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    # Perform mean pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    \n",
    "    # Return the sentence embedding (flattened to 1D tensor)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the embedding function to each row in the DataFrame\n",
    "df_balanced[\"Embeddings\"] = df_balanced[\"Command\"].apply(lambda x: get_embedding(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Storing df_balanced with embeddings in a pickle file\n",
    "with open('df_embeddings.pickle', 'wb') as file:\n",
    "    pickle.dump(df_balanced, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('df_embeddings.pickle', 'rb') as file:\n",
    "    df_balanced = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_balanced.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.MAX_MEM_USAGE_MB.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_balanced['MAX_MEM_USAGE_MB'] > 10000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data(df, test_size=0.2, random_state=42):\n",
    "    X = np.array([np.array([tensor.item() for tensor in emb[0]]) for emb in df[\"Embeddings\"]])\n",
    "    y = np.log1p(df[\"MAX_MEM_USAGE_MB\"].values)\n",
    "    \n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "X_train, X_test, y_train, y_test = prepare_data(df_balanced, test_size=0.2, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "model = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "def evaluate_model(model, X, y, dataset_label=\"\"):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = root_mean_squared_error(y, y_pred) \n",
    "\n",
    "    print(f\"{dataset_label} R²: {r2:.4f}\")\n",
    "    print(f\"{dataset_label} RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return y_pred\n",
    "y_train_pred = evaluate_model(model, X_train, y_train, \"Train\")\n",
    "y_test_pred = evaluate_model(model, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(y_true, y_pred, label=\"\"):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.3, label=label)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--', label='Ideal')\n",
    "    plt.xlabel(\"Actual (log)\")\n",
    "    plt.ylabel(\"Predicted (log)\")\n",
    "    plt.title(f\"{label} - Predicted vs Actual\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_results(y_train, y_train_pred, \"Train\")\n",
    "plot_results(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample predictions (in MB):\")\n",
    "print(\"Predicted:\", np.round(np.expm1(y_test_pred[:5]), 2))\n",
    "print(\"Actual:   \", np.round(np.expm1(y_test[:5]), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
