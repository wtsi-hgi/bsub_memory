{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "This notebook addresses feedback related to data preparation. \n",
    "\n",
    "Key Steps Covered:\n",
    "\n",
    "1. Filtering Small Commands:\n",
    "    \n",
    "    Commands with memory usage less than 1 MB are filtered out.\n",
    "\n",
    "2. Including Outliers:\n",
    "    \n",
    "    The top 0.1% of high-memory usage outliers (previously excluded) are now retained to observe their influence during training and evaluation.\n",
    "\n",
    "3. Log Transformation:\n",
    "    \n",
    "    The MAX_MEM_USAGE_MB column is log-transformed (using np.log10) to reduce skew and compress the range.\n",
    "\n",
    "4. Binning for Stratified Downsampling:\n",
    "    \n",
    "    Two temporary columns are created: log (log-transformed memory usage) and bin (using pd.cut()).\n",
    "    These bins allow us to stratify the data during downsampling, ensuring we retain distribution.\n",
    "    After sampling, temporary columns are dropped.\n",
    "\n",
    "5. Visualisation:\n",
    "    \n",
    "    For visualisation, plots use a log-scaled x-axis when showing the memory usage distribution.\n",
    "    The downsampled dataset appears more balanced, with visible high-memory outliers at the tail retained for analysis.\n",
    "\n",
    "6. Saving Intermediate Results:\n",
    "    \n",
    "    Two dataframe one cleaned and another downsampled are stored in a temporary variable for use in later training and testing notebooks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/Users/dn10/Downloads/Bsub_dataset/filtered_under_5GB.jsonl\", lines=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter jobs with low memory\n",
    "print(f\"length of df before any filtering: {len(df)}\")\n",
    "df_low = df[df[\"MAX_MEM_USAGE_MB\"] < 1.0].copy()\n",
    "df_filtered = df[df[\"MAX_MEM_USAGE_MB\"] >= 1.0].copy()\n",
    "print(f\"length of df after initial filtering: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the filtered rows are with memory usage of 0, therefore safe to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_filtered.copy()\n",
    "df1['log_max_usage'] = np.log10(df_filtered[\"MAX_MEM_USAGE_MB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['log_max_usage'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The log transformed data is not as skewed and provide a good distribution for us to sample from for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the data\n",
    "df2 = df1.copy()\n",
    "df2['bin'] = pd.cut(df1[\"log_max_usage\"], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from each bin\n",
    "df3 = (df2\n",
    "        .groupby('bin')\n",
    "        .apply(lambda x: x.sample(min(len(x),1000), random_state=42))\n",
    "        .drop(columns=['bin', 'log_max_usage'])\n",
    "        .reset_index(drop=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"MAX_MEM_USAGE_MB\"].hist(bins=50)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the df2(dataframe without downsampling) and df3( dataframe with downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "df2_index = df2.index.to_list()\n",
    "with open('/Users/dn10/Downloads/Bsub_dataset/df_without_downsampling.json','w')as f:\n",
    "    json.dump(df2_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/dn10/Downloads/Bsub_dataset/df_without_downsampling.json', 'r') as f:\n",
    "    df4 = pd.Index(json.load(f))\n",
    "index_from_json = df.loc[df4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_json('/Users/dn10/Downloads/Bsub_dataset/df_with_downsampling.json', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
